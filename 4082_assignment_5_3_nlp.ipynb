{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbTwWzvD6NDKvPVSG1jBdC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakeththelu/NLP/blob/main/4082_assignment_5_3_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgVtgELfqGNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4afe4ca5",
        "outputId": "ef6da4ec-e336-4151-a730-59df3780528a"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str): # Handle potential non-string values\n",
        "        return text\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text) # Remove special characters, keep only letters and spaces\n",
        "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with a single space\n",
        "    text = text.strip() # Remove leading/trailing spaces\n",
        "    return text\n",
        "\n",
        "# Apply the function to the 'summaries' column\n",
        "df['cleaned_summaries'] = df['titles'].apply(clean_text)\n",
        "\n",
        "print(\"Original summaries (first 5 rows):\")\n",
        "print(df['titles'].head())\n",
        "print(\"\\nCleaned summaries (first 5 rows):\")\n",
        "print(df['cleaned_summaries'].head())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original summaries (first 5 rows):\n",
            "0    Survey on Semantic Stereo Matching / Semantic ...\n",
            "1    FUTURE-AI: Guiding Principles and Consensus Re...\n",
            "2    Enforcing Mutual Consistency of Hard Regions f...\n",
            "3    Parameter Decoupling Strategy for Semi-supervi...\n",
            "4    Background-Foreground Segmentation for Interio...\n",
            "Name: titles, dtype: object\n",
            "\n",
            "Cleaned summaries (first 5 rows):\n",
            "0    survey on semantic stereo matching semantic de...\n",
            "1    futureai guiding principles and consensus reco...\n",
            "2    enforcing mutual consistency of hard regions f...\n",
            "3    parameter decoupling strategy for semisupervis...\n",
            "4    backgroundforeground segmentation for interior...\n",
            "Name: cleaned_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188090dd"
      },
      "source": [
        "## Word Tokenization (NLTK)\n",
        "\n",
        "### Subtask:\n",
        "Apply NLTK's `word_tokenize` to the 'cleaned_summaries' column to break the text into individual words, storing the tokenized lists in a new 'tokenized_summaries' column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "136cf22e",
        "outputId": "0feaa263-8e3d-48ca-ed80-8deeeff8742c"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Apply word tokenization\n",
        "df['tokenized_summaries'] = df['cleaned_summaries'].apply(word_tokenize)\n",
        "\n",
        "print(\"Tokenized summaries (first 5 rows):\")\n",
        "print(df['tokenized_summaries'].head())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized summaries (first 5 rows):\n",
            "0    [survey, on, semantic, stereo, matching, seman...\n",
            "1    [futureai, guiding, principles, and, consensus...\n",
            "2    [enforcing, mutual, consistency, of, hard, reg...\n",
            "3    [parameter, decoupling, strategy, for, semisup...\n",
            "4    [backgroundforeground, segmentation, for, inte...\n",
            "Name: tokenized_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7701e0ad"
      },
      "source": [
        "## Stopword Removal (NLTK)\n",
        "\n",
        "### Subtask:\n",
        "Filter out common English stopwords from the 'tokenized_summaries' using NLTK's stopwords list, storing the cleaned lists in a new 'filtered_summaries' column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ca8639",
        "outputId": "ca6b766e-bb7c-4627-c18c-10424ee59bf5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the 'stopwords' data if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to remove stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Apply the function to the 'tokenized_summaries' column\n",
        "df['filtered_summaries'] = df['tokenized_summaries'].apply(remove_stopwords)\n",
        "\n",
        "print(\"Filtered summaries (first 5 rows):\")\n",
        "print(df['filtered_summaries'].head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered summaries (first 5 rows):\n",
            "0    [survey, semantic, stereo, matching, semantic,...\n",
            "1    [futureai, guiding, principles, consensus, rec...\n",
            "2    [enforcing, mutual, consistency, hard, regions...\n",
            "3    [parameter, decoupling, strategy, semisupervis...\n",
            "4    [backgroundforeground, segmentation, interior,...\n",
            "Name: filtered_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80fc948b"
      },
      "source": [
        "## Lemmatization (NLTK)\n",
        "\n",
        "### Subtask:\n",
        "Apply NLTK's `WordNetLemmatizer` to reduce words in the 'filtered_summaries' to their base or dictionary form, creating a new 'lemmatized_summaries' column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e79754c",
        "outputId": "e743f40b-5197-4850-e884-e37e4530e22d"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet # Not directly used for the lemmatizer, but good practice to import if related to wordnet\n",
        "\n",
        "# Download the 'wordnet' data if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to lemmatize tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Apply the function to the 'filtered_summaries' column\n",
        "df['lemmatized_summaries'] = df['filtered_summaries'].apply(lemmatize_tokens)\n",
        "\n",
        "print(\"Lemmatized summaries (first 5 rows):\")\n",
        "print(df['lemmatized_summaries'].head())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized summaries (first 5 rows):\n",
            "0    [survey, semantic, stereo, matching, semantic,...\n",
            "1    [futureai, guiding, principle, consensus, reco...\n",
            "2    [enforcing, mutual, consistency, hard, region,...\n",
            "3    [parameter, decoupling, strategy, semisupervis...\n",
            "4    [backgroundforeground, segmentation, interior,...\n",
            "Name: lemmatized_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa6aa9eb",
        "outputId": "7d19ce3b-cddc-4a9a-b172-0a704e7d5245"
      },
      "source": [
        "print('Lemmatization subtask completed successfully.')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization subtask completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e53da26"
      },
      "source": [
        "## Rejoining Words\n",
        "\n",
        "### Subtask:\n",
        "Rejoin the lemmatized words from the 'lemmatized_summaries' column back into a single string for each text entry, storing the result in a new 'rejoined_summaries' column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb2500ae",
        "outputId": "c5927f85-57db-4ed9-87f2-82f6be63cb73"
      },
      "source": [
        "def rejoin_words(word_list):\n",
        "    return ' '.join(word_list)\n",
        "\n",
        "df['rejoined_summaries'] = df['lemmatized_summaries'].apply(rejoin_words)\n",
        "\n",
        "print(\"Rejoined summaries (first 5 rows):\")\n",
        "print(df['rejoined_summaries'].head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rejoined summaries (first 5 rows):\n",
            "0    survey semantic stereo matching semantic depth...\n",
            "1    futureai guiding principle consensus recommend...\n",
            "2    enforcing mutual consistency hard region semis...\n",
            "3    parameter decoupling strategy semisupervised l...\n",
            "4    backgroundforeground segmentation interior sen...\n",
            "Name: rejoined_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f358794"
      },
      "source": [
        "## Unified NLTK Preprocessing Pipeline Function\n",
        "\n",
        "### Subtask:\n",
        "Develop and apply a single comprehensive Python function that encapsulates all NLTK-based preprocessing steps (tokenization, stopword removal, and lemmatization) and apply it to the 'cleaned_summaries' column to create a 'unified_processed_summaries' column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd303b20",
        "outputId": "ec335499-030b-46da-a90b-722111229d88"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Initialize WordNetLemmatizer and get English stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define the unified preprocessing pipeline function\n",
        "def unified_preprocessing_pipeline(text):\n",
        "    # 1. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 2. Stopword Removal\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # 3. Lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply the unified function to the 'cleaned_summaries' column\n",
        "df['unified_processed_summaries'] = df['cleaned_summaries'].apply(unified_preprocessing_pipeline)\n",
        "\n",
        "# Print the first 5 rows of the new column\n",
        "print(\"Unified Processed Summaries (first 5 rows):\")\n",
        "print(df['unified_processed_summaries'].head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unified Processed Summaries (first 5 rows):\n",
            "0    [survey, semantic, stereo, matching, semantic,...\n",
            "1    [futureai, guiding, principle, consensus, reco...\n",
            "2    [enforcing, mutual, consistency, hard, region,...\n",
            "3    [parameter, decoupling, strategy, semisupervis...\n",
            "4    [backgroundforeground, segmentation, interior,...\n",
            "Name: unified_processed_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ba3dd2e",
        "outputId": "d52dff01-4db3-43a5-fe53-16bb181e1a62"
      },
      "source": [
        "print('Unified NLTK preprocessing pipeline subtask completed successfully.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unified NLTK preprocessing pipeline subtask completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b305e48"
      },
      "source": [
        "## Load spaCy Language Model\n",
        "\n",
        "### Subtask:\n",
        "Load the appropriate English language model from spaCy (e.g., 'en_core_web_sm') to enable tokenization, stopword detection, and lemmatization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de663c94",
        "outputId": "9d1af655-b22c-4ede-cd43-2fa02df1faa0"
      },
      "source": [
        "import spacy\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def load_spacy_model(model_name):\n",
        "    try:\n",
        "        nlp = spacy.load(model_name)\n",
        "        print(f\"SpaCy model '{model_name}' loaded successfully.\")\n",
        "        return nlp\n",
        "    except OSError:\n",
        "        print(f\"SpaCy model '{model_name}' not found. Attempting to download...\")\n",
        "        try:\n",
        "            # Use sys.executable to ensure pip is run with the same Python interpreter\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
        "            nlp = spacy.load(model_name)\n",
        "            print(f\"SpaCy model '{model_name}' downloaded and loaded successfully.\")\n",
        "            return nlp\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading or loading spaCy model '{model_name}': {e}\")\n",
        "            sys.exit(1) # Exit if model cannot be loaded\n",
        "\n",
        "# Load the 'en_core_web_sm' model\n",
        "nlp = load_spacy_model('en_core_web_sm')\n",
        "\n",
        "print(f\"NLP object type: {type(nlp)}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy model 'en_core_web_sm' loaded successfully.\n",
            "NLP object type: <class 'spacy.lang.en.English'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3025bf83"
      },
      "source": [
        "## spaCy Tokenization, Stopword Removal, and Lemmatization\n",
        "\n",
        "### Subtask:\n",
        "Process the 'cleaned_summaries' column using the loaded spaCy model. From the resulting Doc objects, extract the raw tokens for 'spacy_tokenized_summaries', filter out stopwords and punctuation for 'spacy_filtered_summaries', and obtain lemmas for 'spacy_lemmatized_summaries'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d37cd92",
        "outputId": "2660b022-b60f-4704-c425-226d2dc839f4"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# 1. Apply the loaded nlp object to create spaCy Doc objects\n",
        "df['spacy_docs'] = df['cleaned_summaries'].apply(nlp)\n",
        "\n",
        "# 2. Extract raw tokens\n",
        "df['spacy_tokenized_summaries'] = df['spacy_docs'].apply(lambda doc: [token.text for token in doc])\n",
        "\n",
        "# 3. Filter out stopwords and punctuation\n",
        "df['spacy_filtered_summaries'] = df['spacy_docs'].apply(lambda doc: [token.text for token in doc if not token.is_stop and not token.is_punct])\n",
        "\n",
        "# 4. Extract lemmas, filtering out stopwords and punctuation\n",
        "df['spacy_lemmatized_summaries'] = df['spacy_docs'].apply(lambda doc: [token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
        "\n",
        "# 5. Print the head of the DataFrame to display the newly created columns\n",
        "print(\"SpaCy Tokenized Summaries (first 5 rows):\")\n",
        "print(df['spacy_tokenized_summaries'].head())\n",
        "print(\"\\nSpaCy Filtered Summaries (first 5 rows):\")\n",
        "print(df['spacy_filtered_summaries'].head())\n",
        "print(\"\\nSpaCy Lemmatized Summaries (first 5 rows):\")\n",
        "print(df['spacy_lemmatized_summaries'].head())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy Tokenized Summaries (first 5 rows):\n",
            "0    [survey, on, semantic, stereo, matching, seman...\n",
            "1    [futureai, guiding, principles, and, consensus...\n",
            "2    [enforcing, mutual, consistency, of, hard, reg...\n",
            "3    [parameter, decoupling, strategy, for, semisup...\n",
            "4    [backgroundforeground, segmentation, for, inte...\n",
            "Name: spacy_tokenized_summaries, dtype: object\n",
            "\n",
            "SpaCy Filtered Summaries (first 5 rows):\n",
            "0    [survey, semantic, stereo, matching, semantic,...\n",
            "1    [futureai, guiding, principles, consensus, rec...\n",
            "2    [enforcing, mutual, consistency, hard, regions...\n",
            "3    [parameter, decoupling, strategy, semisupervis...\n",
            "4    [backgroundforeground, segmentation, interior,...\n",
            "Name: spacy_filtered_summaries, dtype: object\n",
            "\n",
            "SpaCy Lemmatized Summaries (first 5 rows):\n",
            "0    [survey, semantic, stereo, match, semantic, de...\n",
            "1    [futureai, guide, principle, consensus, recomm...\n",
            "2    [enforce, mutual, consistency, hard, region, s...\n",
            "3    [parameter, decouple, strategy, semisupervised...\n",
            "4    [backgroundforeground, segmentation, interior,...\n",
            "Name: spacy_lemmatized_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dbf140b"
      },
      "source": [
        "## Rejoining spaCy Lemmatized Words\n",
        "\n",
        "### Subtask:\n",
        "Rejoin the lemmatized words from the 'spacy_lemmatized_summaries' column back into a single string for each text entry, storing the result in a new 'spacy_rejoined_summaries' column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61f4234e",
        "outputId": "3a222910-865e-42d9-d308-704c034cf712"
      },
      "source": [
        "def rejoin_words(word_list):\n",
        "    return ' '.join(word_list)\n",
        "\n",
        "df['spacy_rejoined_summaries'] = df['spacy_lemmatized_summaries'].apply(rejoin_words)\n",
        "\n",
        "print(\"SpaCy Rejoined Summaries (first 5 rows):\")\n",
        "print(df['spacy_rejoined_summaries'].head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy Rejoined Summaries (first 5 rows):\n",
            "0    survey semantic stereo match semantic depth es...\n",
            "1    futureai guide principle consensus recommendat...\n",
            "2    enforce mutual consistency hard region semisup...\n",
            "3    parameter decouple strategy semisupervised d l...\n",
            "4    backgroundforeground segmentation interior sen...\n",
            "Name: spacy_rejoined_summaries, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5761d56e"
      },
      "source": [
        "## Unified spaCy Preprocessing Pipeline Function\n",
        "\n",
        "### Subtask:\n",
        "Develop and apply a single comprehensive Python function that encapsulates all spaCy-based preprocessing steps (tokenization, stopword removal, and lemmatization) and apply it to the 'cleaned_summaries' column to create a 'unified_spacy_processed_summaries' column. This function will return a list of lemmatized, non-stopword tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e416ecf",
        "outputId": "4e0fec32-4db1-42d4-b098-6505cdadacca"
      },
      "source": [
        "def unified_spacy_pipeline(text):\n",
        "    doc = nlp(text)\n",
        "    # Filter out stopwords and punctuation, and lemmatize\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply the unified function to the 'cleaned_summaries' column\n",
        "df['unified_spacy_processed_summaries'] = df['cleaned_summaries'].apply(unified_spacy_pipeline)\n",
        "\n",
        "# Print the first 5 rows of the new column\n",
        "print(\"Unified spaCy Processed Summaries (first 5 rows):\")\n",
        "print(df['unified_spacy_processed_summaries'].head())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unified spaCy Processed Summaries (first 5 rows):\n",
            "0    [survey, semantic, stereo, match, semantic, de...\n",
            "1    [futureai, guide, principle, consensus, recomm...\n",
            "2    [enforce, mutual, consistency, hard, region, s...\n",
            "3    [parameter, decouple, strategy, semisupervised...\n",
            "4    [backgroundforeground, segmentation, interior,...\n",
            "Name: unified_spacy_processed_summaries, dtype: object\n"
          ]
        }
      ]
    }
  ]
}