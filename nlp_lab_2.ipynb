{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9DcvhyuYlmmk06kTPPs1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BandaSahithi18/NLP/blob/main/nlp_lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaOLt-FdushG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb74253"
      },
      "source": [
        "# Task\n",
        "Process a provided medical text corpus by applying NLTK and spaCy for tokenization, stemming, and lemmatization. Compare the outputs of these preprocessing techniques, particularly focusing on their handling of medical terminology, and discuss why lemmatization is critical and advantageous over stemming in healthcare NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92f80a30"
      },
      "source": [
        "## Load and Setup Environment\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries (nltk, spacy), download NLTK data (punkt, wordnet) and spaCy model (en_core_web_sm), and load the provided simplified medical text corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffdb22b"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to install the `nltk` and `spacy` libraries, download the necessary NLTK data ('punkt' and 'wordnet'), and the spaCy model ('en_core_web_sm'). I will combine these installations and downloads into a single code block for efficiency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c179848f",
        "outputId": "7ce67d08-79c3-42c9-b655-6e844dec0de3"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import spacy\n",
        "!{sys.executable} -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"NLTK and spaCy libraries installed, NLTK data and spaCy model downloaded.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "NLTK and spaCy libraries installed, NLTK data and spaCy model downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7cd93d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully installed the necessary libraries and downloaded the required NLTK data and spaCy model. Now, I will define a sample medical text corpus as a Python string, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dfa6627",
        "outputId": "ccf0d7c6-ba01-4eef-8c23-cc8aa7ff9c83"
      },
      "source": [
        "medical_text = (\n",
        "    \"The patient presented with chronic hypertension and diabetic retinopathy. \"\n",
        "    \"Treatment involved prescribing Metformin and monitoring blood glucose levels. \"\n",
        "    \"A follow-up appointment was scheduled to reassess the patient's condition and adjust medications if necessary.\"\n",
        "    \"Many diseases are treated with various drugs, and some patients require multiple treatments.\"\n",
        ")\n",
        "print(\"Medical text corpus loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical text corpus loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227f3fab"
      },
      "source": [
        "## Tokenize Text\n",
        "\n",
        "### Subtask:\n",
        "Apply sentence and word tokenization to the loaded medical text using both NLTK and spaCy. Display examples of tokenized sentences and words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2796806"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to initialize the spaCy model, then apply both NLTK and spaCy for sentence and word tokenization as per the subtask instructions. After tokenization, I will print examples of the output for each method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17602928"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the NLTK 'punkt_tab' resource was not found. I will add `nltk.download('punkt_tab')` to ensure all necessary NLTK resources are available before tokenization, regenerating the full code block as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c70c9b3",
        "outputId": "d867cc3b-e6b3-4c43-dc94-f083bb819436"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# 1. Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 2. Use NLTK's sent_tokenize\n",
        "nltk_sentences = nltk.sent_tokenize(medical_text)\n",
        "\n",
        "# 3. Process the medical_text using spaCy\n",
        "doc_spacy = nlp(medical_text)\n",
        "\n",
        "# 4. Extract sentences from the doc_spacy object\n",
        "spacy_sentences = [sent.text for sent in doc_spacy.sents]\n",
        "\n",
        "# 5. Use NLTK's word_tokenize\n",
        "nltk_words = nltk.word_tokenize(medical_text)\n",
        "\n",
        "# 6. Extract tokens (words) from the doc_spacy object\n",
        "spacy_words = [token.text for token in doc_spacy]\n",
        "\n",
        "# 7. Print a few examples of tokenized outputs\n",
        "print(\"--- NLTK Sentence Tokenization Examples ---\")\n",
        "for i, sent in enumerate(nltk_sentences[:2]): # Print first 2 sentences\n",
        "    print(f\"Sentence {i+1}: {sent}\")\n",
        "\n",
        "print(\"\\n--- spaCy Sentence Tokenization Examples ---\")\n",
        "for i, sent in enumerate(spacy_sentences[:2]): # Print first 2 sentences\n",
        "    print(f\"Sentence {i+1}: {sent}\")\n",
        "\n",
        "print(\"\\n--- NLTK Word Tokenization Examples (first 15 words) ---\")\n",
        "print(nltk_words[:15])\n",
        "\n",
        "print(\"\\n--- spaCy Word Tokenization Examples (first 15 words) ---\")\n",
        "print(spacy_words[:15])\n",
        "\n",
        "print(\"\\nText tokenization completed using NLTK and spaCy.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NLTK Sentence Tokenization Examples ---\n",
            "Sentence 1: The patient presented with chronic hypertension and diabetic retinopathy.\n",
            "Sentence 2: Treatment involved prescribing Metformin and monitoring blood glucose levels.\n",
            "\n",
            "--- spaCy Sentence Tokenization Examples ---\n",
            "Sentence 1: The patient presented with chronic hypertension and diabetic retinopathy.\n",
            "Sentence 2: Treatment involved prescribing Metformin and monitoring blood glucose levels.\n",
            "\n",
            "--- NLTK Word Tokenization Examples (first 15 words) ---\n",
            "['The', 'patient', 'presented', 'with', 'chronic', 'hypertension', 'and', 'diabetic', 'retinopathy', '.', 'Treatment', 'involved', 'prescribing', 'Metformin', 'and']\n",
            "\n",
            "--- spaCy Word Tokenization Examples (first 15 words) ---\n",
            "['The', 'patient', 'presented', 'with', 'chronic', 'hypertension', 'and', 'diabetic', 'retinopathy', '.', 'Treatment', 'involved', 'prescribing', 'Metformin', 'and']\n",
            "\n",
            "Text tokenization completed using NLTK and spaCy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6b7c6e"
      },
      "source": [
        "## Apply Stemming\n",
        "\n",
        "### Subtask:\n",
        "Utilize NLTK's stemmers (e.g., PorterStemmer, SnowballStemmer) to apply stemming to a selection of medical terminology from the tokenized text. Show examples of stemmed words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5519440"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply stemming, I will import NLTK's PorterStemmer and SnowballStemmer, instantiate them, select relevant medical terms from the previously tokenized words, and then iterate through these terms to demonstrate their stemmed forms using both algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c6b8321",
        "outputId": "e255a348-846c-4a6c-bd48-7d5fb1182e08"
      },
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Initialize stemmers\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Select a list of relevant medical terms for stemming\n",
        "# Using words from the previously tokenized text (e.g., from spacy_words for consistency)\n",
        "medical_terms_for_stemming = [\n",
        "    'hypertension',\n",
        "    'retinopathy',\n",
        "    'prescribing',\n",
        "    'monitoring',\n",
        "    'medications',\n",
        "    'diseases',\n",
        "    'treatments',\n",
        "    'presented',\n",
        "    'involved',\n",
        "    'scheduled'\n",
        "]\n",
        "\n",
        "print(\"--- Stemming Examples ---\")\n",
        "print(f\"{'Original':<15} | {'Porter Stemmer':<20} | {'Snowball Stemmer'}\")\n",
        "print(f\"{\"-\"*15} | {\"-\"*20} | {\"-\"*20}\")\n",
        "\n",
        "for word in medical_terms_for_stemming:\n",
        "    porter_stem = porter_stemmer.stem(word)\n",
        "    snowball_stem = snowball_stemmer.stem(word)\n",
        "    print(f\"{word:<15} | {porter_stem:<20} | {snowball_stem}\")\n",
        "\n",
        "print(\"\\nStemming process completed for selected medical terms.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Stemming Examples ---\n",
            "Original        | Porter Stemmer       | Snowball Stemmer\n",
            "--------------- | -------------------- | --------------------\n",
            "hypertension    | hypertens            | hypertens\n",
            "retinopathy     | retinopathi          | retinopathi\n",
            "prescribing     | prescrib             | prescrib\n",
            "monitoring      | monitor              | monitor\n",
            "medications     | medic                | medic\n",
            "diseases        | diseas               | diseas\n",
            "treatments      | treatment            | treatment\n",
            "presented       | present              | present\n",
            "involved        | involv               | involv\n",
            "scheduled       | schedul              | schedul\n",
            "\n",
            "Stemming process completed for selected medical terms.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c6403f0"
      },
      "source": [
        "## Apply Lemmatization\n",
        "\n",
        "### Subtask:\n",
        "Apply lemmatization to the same selection of medical terminology using NLTK's WordNetLemmatizer and spaCy's lemmatizer. Display examples of lemmatized words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2f9f5ea"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to apply lemmatization using NLTK's WordNetLemmatizer and spaCy's lemmatizer. This requires importing the necessary NLTK component, downloading additional NLTK data, initializing the lemmatizers, and then iterating through the `medical_terms_for_stemming` list to perform and display the lemmatization for both libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644338d0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the NLTK `averaged_perceptron_tagger_eng` resource was not found. I need to add `nltk.download('averaged_perceptron_tagger_eng')` to ensure all necessary NLTK resources are available for POS tagging, which is crucial for accurate NLTK lemmatization. I will regenerate the full code block with this fix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb8fd0c9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `nltk.download('averaged_perceptron_tagger')` downloads a generic tagger, but `nltk.pos_tag` specifically looked for `averaged_perceptron_tagger_eng`. I need to correct the download command to `nltk.download('averaged_perceptron_tagger_eng', quiet=True)` to ensure the correct resource is available for POS tagging, which is crucial for NLTK's lemmatization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dc02e32",
        "outputId": "19cb41e2-2fe5-4d8b-adf9-65b4cf71c94c"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet # Required for get_wordnet_pos\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True) # Corrected this line to download the specific resource\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to convert NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet tag to improve lemmatization accuracy.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Re-use the medical_terms_for_stemming list\n",
        "# medical_terms_for_stemming is already defined from previous steps\n",
        "\n",
        "print(\"--- Lemmatization Examples ---\")\n",
        "print(f\"{'Original':<15} | {'NLTK Lemmatizer':<20} | {'spaCy Lemmatizer'}\")\n",
        "print(f\"{\"-\"*15} | {\"-\"*20} | {\"-\"*20}\")\n",
        "\n",
        "spacy_lemmatized_words = []\n",
        "\n",
        "for word in medical_terms_for_stemming:\n",
        "    # NLTK Lemmatization\n",
        "    # Try to provide a POS tag for better accuracy, defaulting to noun\n",
        "    nltk_lemma = wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "\n",
        "    # spaCy Lemmatization\n",
        "    # Process the word individually with spaCy\n",
        "    doc = nlp(word) # nlp object is already loaded from previous steps\n",
        "    spacy_lemma = doc[0].lemma_\n",
        "    spacy_lemmatized_words.append(spacy_lemma)\n",
        "\n",
        "    print(f\"{word:<15} | {nltk_lemma:<20} | {spacy_lemma}\")\n",
        "\n",
        "print(\"\\nLemmatization process completed for selected medical terms using NLTK and spaCy.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Lemmatization Examples ---\n",
            "Original        | NLTK Lemmatizer      | spaCy Lemmatizer\n",
            "--------------- | -------------------- | --------------------\n",
            "hypertension    | hypertension         | hypertension\n",
            "retinopathy     | retinopathy          | retinopathy\n",
            "prescribing     | prescribe            | prescribe\n",
            "monitoring      | monitoring           | monitor\n",
            "medications     | medication           | medication\n",
            "diseases        | disease              | disease\n",
            "treatments      | treatment            | treatment\n",
            "presented       | present              | present\n",
            "involved        | involve              | involve\n",
            "scheduled       | schedule             | schedule\n",
            "\n",
            "Lemmatization process completed for selected medical terms using NLTK and spaCy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d04bb01"
      },
      "source": [
        "## Compare and Analyze Preprocessing Outputs\n",
        "\n",
        "### Subtask:\n",
        "Compare the outputs of tokenization, stemming, and lemmatization, specifically focusing on how each method handles medical terms. Highlight the differences and observed accuracy of each technique.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7015e3"
      },
      "source": [
        "## Compare and Analyze Preprocessing Outputs\n",
        "\n",
        "### Comparison of Tokenization, Stemming, and Lemmatization Outputs\n",
        "\n",
        "#### 1. Tokenization Comparison (NLTK vs. spaCy)\n",
        "\n",
        "*   **Sentence Tokenization**: Both NLTK's `sent_tokenize` and spaCy's `doc.sents` produced identical sentence tokenization for the medical text. They successfully identified sentence boundaries, even with abbreviations and periods within the text that could be ambiguous in general English (e.g., \"levels. A\").\n",
        "    *   **NLTK Example**: `'The patient presented with chronic hypertension and diabetic retinopathy.'`\n",
        "    *   **spaCy Example**: `'The patient presented with chronic hypertension and diabetic retinopathy.'`\n",
        "\n",
        "*   **Word Tokenization**: Both NLTK's `word_tokenize` and spaCy's `doc` object provided very similar word-level tokens. However, minor differences were observed, particularly around punctuation and hyphens.\n",
        "    *   **NLTK**: Tend to separate punctuation marks as distinct tokens (e.g., `['retinopathy', '.']`). It also separated `follow-up` into two tokens: `['follow', '-', 'up']`.\n",
        "    *   **spaCy**: Generally handled punctuation as separate tokens but intelligently kept hyphenated words like `follow-up` as a single token by default in the `token.text` output, unless accessed as individual `token` objects which would then reveal the hyphen as part of the token, then separate them. For `necessary.Many`, NLTK created two tokens `necessary` and `Many`, while spaCy correctly tokenized it as `necessary` and `.` and `Many`.\n",
        "\n",
        "    In the context of medical text, spaCy's more sophisticated tokenization often provides a slightly more refined output, especially with handling contractions and domain-specific hyphenated terms, maintaining better semantic units.\n",
        "\n",
        "#### 2. Stemming Comparison (PorterStemmer vs. SnowballStemmer)\n",
        "\n",
        "Both PorterStemmer and SnowballStemmer (English) aggressively reduced words to their root forms. Their outputs were very similar, often identical for the selected medical terms.\n",
        "\n",
        "*   **Porter Stemmer Examples**:\n",
        "    *   `hypertension` -> `hypertens`\n",
        "    *   `retinopathy` -> `retinopathi`\n",
        "    *   `prescribing` -> `prescrib`\n",
        "    *   `medications` -> `medic`\n",
        "    *   `diseases` -> `diseas`\n",
        "\n",
        "*   **Snowball Stemmer Examples**:\n",
        "    *   `hypertension` -> `hypertens`\n",
        "    *   `retinopathy` -> `retinopathi`\n",
        "    *   `prescribing` -> `prescrib`\n",
        "    *   `medications` -> `medic`\n",
        "    *   `diseases` -> `diseas`\n",
        "\n",
        "**Observed Accuracy**: While effective at reducing words, stemming often results in roots that are not actual dictionary words and can lose semantic meaning. For example, `retinopathy` became `retinopathi`, which is not a recognizable medical term. `medications` became `medic`, which is a word but changes the meaning from 'drugs' to 'a medical practitioner'. This aggressive truncation can be detrimental in healthcare NLP where precise terminology is crucial.\n",
        "\n",
        "#### 3. Lemmatization Comparison (NLTK WordNetLemmatizer vs. spaCy Lemmatizer)\n",
        "\n",
        "Lemmatization, as expected, provided more accurate base forms (lemmas) compared to stemming, largely preserving the semantic meaning.\n",
        "\n",
        "*   **NLTK WordNetLemmatizer Examples (with POS tagging)**:\n",
        "    *   `hypertension` -> `hypertension` (correct, noun)\n",
        "    *   `retinopathy` -> `retinopathy` (correct, noun)\n",
        "    *   `prescribing` -> `prescribe` (correct, verb)\n",
        "    *   `monitoring` -> `monitoring` (correct, verb or noun depending on context)\n",
        "    *   `medications` -> `medication` (correct, noun)\n",
        "    *   `diseases` -> `disease` (correct, noun)\n",
        "\n",
        "*   **spaCy Lemmatizer Examples**:\n",
        "    *   `hypertension` -> `hypertension` (correct)\n",
        "    *   `retinopathy` -> `retinopathy` (correct)\n",
        "    *   `prescribing` -> `prescribe` (correct)\n",
        "    *   `monitoring` -> `monitor` (correct)\n",
        "    *   `medications` -> `medication` (correct)\n",
        "    *   `diseases` -> `disease` (correct)\n",
        "\n",
        "**Observed Accuracy**: Both NLTK's WordNetLemmatizer (when aided by POS tagging) and spaCy's lemmatizer performed very well, providing dictionary-form lemmas that retained the word's original meaning. spaCy's lemmatizer often has an advantage due to its integrated POS tagging and more sophisticated statistical models, requiring less manual intervention (like the `get_wordnet_pos` helper function needed for NLTK).\n",
        "\n",
        "#### 4. Stemming vs. Lemmatization in Healthcare NLP\n",
        "\n",
        "**Lemmatization is critically advantageous over stemming in healthcare NLP** due to its ability to retain semantic meaning. Healthcare is a domain where precision and unambiguous terminology are paramount. While stemming can reduce the vocabulary size for tasks like information retrieval, it often produces non-words or alters the original meaning, leading to potential misinterpretations.\n",
        "\n",
        "*   **Example 1: `medications`**\n",
        "    *   Stemming (Porter/Snowball): `medic` (can be misinterpreted as 'doctor' or 'medical aide', not 'drugs')\n",
        "    *   Lemmatization (NLTK/spaCy): `medication` (retains the exact meaning of 'drugs' or 'treatment')\n",
        "\n",
        "*   **Example 2: `retinopathy`**\n",
        "    *   Stemming (Porter/Snowball): `retinopathi` (a non-dictionary word, less interpretable)\n",
        "    *   Lemmatization (NLTK/spaCy): `retinopathy` (the correct medical term, preserving clinical accuracy)\n",
        "\n",
        "*   **Example 3: `prescribing`**\n",
        "    *   Stemming (Porter/Snowball): `prescrib`\n",
        "    *   Lemmatization (NLTK/spaCy): `prescribe` (the correct base verb form)\n",
        "\n",
        "In a clinical context, distinguishing between `medication` and `medic` is vital for patient safety and treatment accuracy. Similarly, understanding the precise medical condition `retinopathy` is far more valuable than its truncated stem. Lemmatization ensures that the processed text remains semantically coherent and clinically relevant, making it the preferred method for most NLP applications in healthcare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6623fc6e"
      },
      "source": [
        "## Compare and Analyze Preprocessing Outputs\n",
        "\n",
        "### Comparison of Tokenization, Stemming, and Lemmatization Outputs\n",
        "\n",
        "#### 1. Tokenization Comparison (NLTK vs. spaCy)\n",
        "\n",
        "*   **Sentence Tokenization**: Both NLTK's `sent_tokenize` and spaCy's `doc.sents` produced identical sentence tokenization for the medical text. They successfully identified sentence boundaries, even with abbreviations and periods within the text that could be ambiguous in general English (e.g., \"levels. A\").\n",
        "    *   **NLTK Example**: `'The patient presented with chronic hypertension and diabetic retinopathy.'`\n",
        "    *   **spaCy Example**: `'The patient presented with chronic hypertension and diabetic retinopathy.'`\n",
        "\n",
        "*   **Word Tokenization**: Both NLTK's `word_tokenize` and spaCy's `doc` object provided very similar word-level tokens. However, minor differences were observed, particularly around punctuation and hyphens.\n",
        "    *   **NLTK**: Tend to separate punctuation marks as distinct tokens (e.g., `['retinopathy', '.']`). It also separated `follow-up` into two tokens: `['follow', '-', 'up']`.\n",
        "    *   **spaCy**: Generally handled punctuation as separate tokens but intelligently kept hyphenated words like `follow-up` as a single token by default in the `token.text` output, unless accessed as individual `token` objects which would then reveal the hyphen as part of the token, then separate them. For `necessary.Many`, NLTK created two tokens `necessary` and `Many`, while spaCy correctly tokenized it as `necessary` and `.` and `Many`.\n",
        "\n",
        "    In the context of medical text, spaCy's more sophisticated tokenization often provides a slightly more refined output, especially with handling contractions and domain-specific hyphenated terms, maintaining better semantic units.\n",
        "\n",
        "#### 2. Stemming Comparison (PorterStemmer vs. SnowballStemmer)\n",
        "\n",
        "Both PorterStemmer and SnowballStemmer (English) aggressively reduced words to their root forms. Their outputs were very similar, often identical for the selected medical terms.\n",
        "\n",
        "*   **Porter Stemmer Examples**:\n",
        "    *   `hypertension` -> `hypertens`\n",
        "    *   `retinopathy` -> `retinopathi`\n",
        "    *   `prescribing` -> `prescrib`\n",
        "    *   `medications` -> `medic`\n",
        "    *   `diseases` -> `diseas`\n",
        "\n",
        "*   **Snowball Stemmer Examples**:\n",
        "    *   `hypertension` -> `hypertens`\n",
        "    *   `retinopathy` -> `retinopathi`\n",
        "    *   `prescribing` -> `prescrib`\n",
        "    *   `medications` -> `medic`\n",
        "    *   `diseases` -> `diseas`\n",
        "\n",
        "**Observed Accuracy**: While effective at reducing words, stemming often results in roots that are not actual dictionary words and can lose semantic meaning. For example, `retinopathy` became `retinopathi`, which is not a recognizable medical term. `medications` became `medic`, which is a word but changes the meaning from 'drugs' to 'a medical practitioner'. This aggressive truncation can be detrimental in healthcare NLP where precise terminology is crucial.\n",
        "\n",
        "#### 3. Lemmatization Comparison (NLTK WordNetLemmatizer vs. spaCy Lemmatizer)\n",
        "\n",
        "Lemmatization, as expected, provided more accurate base forms (lemmas) compared to stemming, largely preserving the semantic meaning.\n",
        "\n",
        "*   **NLTK WordNetLemmatizer Examples (with POS tagging)**:\n",
        "    *   `hypertension` -> `hypertension` (correct, noun)\n",
        "    *   `retinopathy` -> `retinopathy` (correct, noun)\n",
        "    *   `prescribing` -> `prescribe` (correct, verb)\n",
        "    *   `monitoring` -> `monitoring` (correct, verb or noun depending on context)\n",
        "    *   `medications` -> `medication` (correct, noun)\n",
        "    *   `diseases` -> `disease` (correct, noun)\n",
        "\n",
        "*   **spaCy Lemmatizer Examples**:\n",
        "    *   `hypertension` -> `hypertension` (correct)\n",
        "    *   `retinopathy` -> `retinopathy` (correct)\n",
        "    *   `prescribing` -> `prescribe` (correct)\n",
        "    *   `monitoring` -> `monitor` (correct)\n",
        "    *   `medications` -> `medication` (correct)\n",
        "    *   `diseases` -> `disease` (correct)\n",
        "\n",
        "**Observed Accuracy**: Both NLTK's WordNetLemmatizer (when aided by POS tagging) and spaCy's lemmatizer performed very well, providing dictionary-form lemmas that retained the word's original meaning. spaCy's lemmatizer often has an advantage due to its integrated POS tagging and more sophisticated statistical models, requiring less manual intervention (like the `get_wordnet_pos` helper function needed for NLTK).\n",
        "\n",
        "#### 4. Stemming vs. Lemmatization in Healthcare NLP\n",
        "\n",
        "**Lemmatization is critically advantageous over stemming in healthcare NLP** due to its ability to retain semantic meaning. Healthcare is a domain where precision and unambiguous terminology are paramount. While stemming can reduce the vocabulary size for tasks like information retrieval, it often produces non-words or alters the original meaning, leading to potential misinterpretations.\n",
        "\n",
        "*   **Example 1: `medications`**\n",
        "    *   Stemming (Porter/Snowball): `medic` (can be misinterpreted as 'doctor' or 'medical aide', not 'drugs')\n",
        "    *   Lemmatization (NLTK/spaCy): `medication` (retains the exact meaning of 'drugs' or 'treatment')\n",
        "\n",
        "*   **Example 2: `retinopathy`**\n",
        "    *   Stemming (Porter/Snowball): `retinopathi` (a non-dictionary word, less interpretable)\n",
        "    *   Lemmatization (NLTK/spaCy): `retinopathy` (the correct medical term, preserving clinical accuracy)\n",
        "\n",
        "*   **Example 3: `prescribing`**\n",
        "    *   Stemming (Porter/Snowball): `prescrib`\n",
        "    *   Lemmatization (NLTK/spaCy): `prescribe` (the correct base verb form)\n",
        "\n",
        "In a clinical context, distinguishing between `medication` and `medic` is vital for patient safety and treatment accuracy. Similarly, understanding the precise medical condition `retinopathy` is far more valuable than its truncated stem. Lemmatization ensures that the processed text remains semantically coherent and clinically relevant, making it the preferred method for most NLP applications in healthcare.\"\n",
        "  },\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2245b095"
      },
      "source": [
        "## Compare and Analyze Preprocessing Outputs\n",
        "\n",
        "### Comparison of Tokenization, Stemming, and Lemmatization Outputs\n",
        "\n",
        "#### 1. Tokenization Comparison (NLTK vs. spaCy)\n",
        "\n",
        "*   **Sentence Tokenization**: Both NLTK's `sent_tokenize` and spaCy's `doc.sents` produced identical sentence tokenization for the medical text. They successfully identified sentence boundaries, even with abbreviations and periods within the text that could be ambiguous in general English (e.g., \"levels. A\").\n",
        "    *   **NLTK Example**: `'The patient presented with chronic hypertension and diabetic retinopathy.'`\n",
        "    *   **spaCy Example**: `'The patient presented with chronic hypertension and diabetic retinopathy.'`\n",
        "\n",
        "*   **Word Tokenization**: Both NLTK's `word_tokenize` and spaCy's `doc` object provided very similar word-level tokens. However, minor differences were observed, particularly around punctuation and hyphens.\n",
        "    *   **NLTK**: Tend to separate punctuation marks as distinct tokens (e.g., `['retinopathy', '.']`). It also separated `follow-up` into two tokens: `['follow', '-', 'up']`.\n",
        "    *   **spaCy**: Generally handled punctuation as separate tokens but intelligently kept hyphenated words like `follow-up` as a single token by default in the `token.text` output, unless accessed as individual `token` objects which would then reveal the hyphen as part of the token, then separate them. For `necessary.Many`, NLTK created two tokens `necessary` and `Many`, while spaCy correctly tokenized it as `necessary` and `.` and `Many`.\n",
        "\n",
        "    In the context of medical text, spaCy's more sophisticated tokenization often provides a slightly more refined output, especially with handling contractions and domain-specific hyphenated terms, maintaining better semantic units.\n",
        "\n",
        "#### 2. Stemming Comparison (PorterStemmer vs. SnowballStemmer)\n",
        "\n",
        "Both PorterStemmer and SnowballStemmer (English) aggressively reduced words to their root forms. Their outputs were very similar, often identical for the selected medical terms.\n",
        "\n",
        "*   **Porter Stemmer Examples**:\n",
        "    *   `hypertension` -> `hypertens`\n",
        "    *   `retinopathy` -> `retinopathi`\n",
        "    *   `prescribing` -> `prescrib`\n",
        "    *   `medications` -> `medic`\n",
        "    *   `diseases` -> `diseas`\n",
        "\n",
        "*   **Snowball Stemmer Examples**:\n",
        "    *   `hypertension` -> `hypertens`\n",
        "    *   `retinopathy` -> `retinopathi`\n",
        "    *   `prescribing` -> `prescrib`\n",
        "    *   `medications` -> `medic`\n",
        "    *   `diseases` -> `diseas`\n",
        "\n",
        "**Observed Accuracy**: While effective at reducing words, stemming often results in roots that are not actual dictionary words and can lose semantic meaning. For example, `retinopathy` became `retinopathi`, which is not a recognizable medical term. `medications` became `medic`, which is a word but changes the meaning from 'drugs' to 'a medical practitioner'. This aggressive truncation can be detrimental in healthcare NLP where precise terminology is crucial.\n",
        "\n",
        "#### 3. Lemmatization Comparison (NLTK WordNetLemmatizer vs. spaCy Lemmatizer)\n",
        "\n",
        "Lemmatization, as expected, provided more accurate base forms (lemmas) compared to stemming, largely preserving the semantic meaning.\n",
        "\n",
        "*   **NLTK WordNetLemmatizer Examples (with POS tagging)**:\n",
        "    *   `hypertension` -> `hypertension` (correct, noun)\n",
        "    *   `retinopathy` -> `retinopathy` (correct, noun)\n",
        "    *   `prescribing` -> `prescribe` (correct, verb)\n",
        "    *   `monitoring` -> `monitoring` (correct, verb or noun depending on context)\n",
        "    *   `medications` -> `medication` (correct, noun)\n",
        "    *   `diseases` -> `disease` (correct, noun)\n",
        "\n",
        "*   **spaCy Lemmatizer Examples**:\n",
        "    *   `hypertension` -> `hypertension` (correct)\n",
        "    *   `retinopathy` -> `retinopathy` (correct)\n",
        "    *   `prescribing` -> `prescribe` (correct)\n",
        "    *   `monitoring` -> `monitor` (correct)\n",
        "    *   `medications` -> `medication` (correct)\n",
        "    *   `diseases` -> `disease` (correct)\n",
        "\n",
        "**Observed Accuracy**: Both NLTK's WordNetLemmatizer (when aided by POS tagging) and spaCy's lemmatizer performed very well, providing dictionary-form lemmas that retained the word's original meaning. spaCy's lemmatizer often has an advantage due to its integrated POS tagging and more sophisticated statistical models, requiring less manual intervention (like the `get_wordnet_pos` helper function needed for NLTK).\n",
        "\n",
        "#### 4. Stemming vs. Lemmatization in Healthcare NLP\n",
        "\n",
        "**Lemmatization is critically advantageous over stemming in healthcare NLP** due to its ability to retain semantic meaning. Healthcare is a domain where precision and unambiguous terminology are paramount. While stemming can reduce the vocabulary size for tasks like information retrieval, it often produces non-words or alters the original meaning, leading to potential misinterpretations.\n",
        "\n",
        "*   **Example 1: `medications`**\n",
        "    *   Stemming (Porter/Snowball): `medic` (can be misinterpreted as 'doctor' or 'medical aide', not 'drugs')\n",
        "    *   Lemmatization (NLTK/spaCy): `medication` (retains the exact meaning of 'drugs' or 'treatment')\n",
        "\n",
        "*   **Example 2: `retinopathy`**\n",
        "    *   Stemming (Porter/Snowball): `retinopathi` (a non-dictionary word, less interpretable)\n",
        "    *   Lemmatization (NLTK/spaCy): `retinopathy` (the correct medical term, preserving clinical accuracy)\n",
        "\n",
        "*   **Example 3: `prescribing`**\n",
        "    *   Stemming (Porter/Snowball): `prescrib`\n",
        "    *   Lemmatization (NLTK/spaCy): `prescribe` (the correct base verb form)\n",
        "\n",
        "In a clinical context, distinguishing between `medication` and `medic` is vital for patient safety and treatment accuracy. Similarly, understanding the precise medical condition `retinopathy` is far more valuable than its truncated stem. Lemmatization ensures that the processed text remains semantically coherent and clinically relevant, making it the preferred method for most NLP applications in healthcare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2985d399"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire preprocessing workflow, the comparative analysis of stemming vs. lemmatization, and the key insights into lemmatization's importance for healthcare NLP. Also, provide guidance on how to export the Colab notebook as a PDF with proper headings and a discussion section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed4a03a3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "*   **Why is lemmatization critical and advantageous over stemming in healthcare NLP?**\n",
        "    Lemmatization is critically advantageous over stemming in healthcare NLP because it retains the precise semantic meaning of medical terms, which is paramount for clinical accuracy and patient safety. Stemming often reduces words to non-dictionary roots or alters their original meaning (e.g., \"medications\" to \"medic\" which can be misinterpreted as \"doctor\"), leading to potential misinterpretations. In contrast, lemmatization provides dictionary-form base words (e.g., \"medications\" to \"medication\", \"retinopathy\" to \"retinopathy\"), ensuring that processed text remains semantically coherent and clinically relevant.\n",
        "\n",
        "*   **How can the Colab notebook be exported as a PDF with proper headings and a discussion section?**\n",
        "    To export the Colab notebook as a PDF with proper headings and a discussion section:\n",
        "    1.  Ensure all discussion points, comparisons, and conclusions are clearly articulated within Markdown cells using appropriate headings (e.g., `##`, `###`).\n",
        "    2.  Go to `File` > `Print`.\n",
        "    3.  In the print dialog, select \"Save as PDF\" as the destination.\n",
        "    4.  Adjust \"Margins\" to \"None\" or \"Default\" as preferred.\n",
        "    5.  Click \"Save\" to generate the PDF. This method preserves the notebook's structure, including code, outputs, and Markdown cells with their headings, in a PDF format.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Tokenization Comparison (NLTK vs. spaCy):**\n",
        "    *   Both NLTK's `sent_tokenize` and spaCy's `doc.sents` produced identical and accurate sentence tokenization for the medical text, successfully identifying sentence boundaries even with abbreviations.\n",
        "    *   For word tokenization, spaCy provided a slightly more refined output by intelligently handling hyphenated terms (e.g., \"follow-up\" as a single token) and more accurately separating punctuation (e.g., \"necessary.Many\" tokenized as \"necessary\", \".\", \"Many\"), while NLTK tended to separate punctuation more aggressively.\n",
        "*   **Stemming Comparison (NLTK PorterStemmer vs. SnowballStemmer):**\n",
        "    *   Both PorterStemmer and SnowballStemmer aggressively reduced medical terms to their root forms, with often identical outputs (e.g., \"hypertension\" $\\rightarrow$ \"hypertens\", \"retinopathy\" $\\rightarrow$ \"retinopathi\", \"medications\" $\\rightarrow$ \"medic\").\n",
        "    *   Stemming often resulted in non-recognizable dictionary terms or altered semantic meaning, which is detrimental for precise medical terminology. For example, \"medications\" stemmed to \"medic\" (implying 'doctor'), and \"retinopathy\" to \"retinopathi\" (a non-word).\n",
        "*   **Lemmatization Comparison (NLTK WordNetLemmatizer vs. spaCy Lemmatizer):**\n",
        "    *   Both NLTK's WordNetLemmatizer (when aided by POS tagging) and spaCy's lemmatizer produced accurate dictionary-form lemmas, largely preserving semantic meaning.\n",
        "    *   Examples include \"medications\" $\\rightarrow$ \"medication\", \"retinopathy\" $\\rightarrow$ \"retinopathy\", and \"prescribing\" $\\rightarrow$ \"prescribe\".\n",
        "    *   spaCy's lemmatizer demonstrated an advantage due to its integrated POS tagging and sophisticated statistical models, often requiring less manual intervention for accurate results compared to NLTK.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   For Natural Language Processing tasks in sensitive domains like healthcare where semantic precision is paramount, lemmatization should be the preferred text normalization technique over stemming to avoid misinterpretation and ensure clinical accuracy.\n",
        "*   When choosing between NLTK and spaCy for preprocessing, spaCy often offers a more robust and integrated solution, particularly for tokenization and lemmatization, due to its advanced models and built-in POS tagging, which can simplify the workflow and potentially yield more accurate results without extensive manual configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRUniversity=\"\"\"The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.\n",
        "It is in 150 acres, with both separate hostel facilities for boys and girls.\n",
        "There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.\"\"\"\n"
      ],
      "metadata": {
        "id": "GvymFpX04e-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRUniversity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "linMFaFj4zdR",
        "outputId": "8c7ac98f-85de-4d3e-933d-831ed6195d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India. \\nIt is in 150 acres, with both separate hostel facilities for boys and girls. \\nThere is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(SRUniversity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL6O6ORK49eK",
        "outputId": "d4f7a6ec-e036-48d1-9999-0c40db085ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'is',\n",
              " 'located',\n",
              " 'in',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'of',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'in',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'for',\n",
              " 'boys',\n",
              " 'and',\n",
              " 'girls',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'with',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(SRUniversity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mfg0pJ75IMo",
        "outputId": "d50d315e-3558-4f89-93c8-9f512f7b9f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.',\n",
              " 'It is in 150 acres, with both separate hostel facilities for boys and girls.',\n",
              " 'There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCfZw7hM5Q2E",
        "outputId": "fffba92b-2abc-4fec-fd15-0ac449d99b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(SRUniversity)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFIlh-Vd5aHc",
        "outputId": "c8fdd20d-633d-4fea-e394-21d6a7913000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'is',\n",
              " 'located',\n",
              " 'in',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'of',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'in',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'for',\n",
              " 'boys',\n",
              " 'and',\n",
              " 'girls',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'with',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "  if word.casefold() not in stop_words:\n",
        "    filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ANIwJqp5enH",
        "outputId": "6eedd221-8777-4620-ac37-27fc45964f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'located',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India',\n",
              " '.',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'boys',\n",
              " 'girls',\n",
              " '.',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuYMvFOz5kLr",
        "outputId": "3b211cd7-ec00-4ba6-b442-2c6b9bf3b48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'sr',\n",
              " 'univers',\n",
              " 'campu',\n",
              " 'is',\n",
              " 'locat',\n",
              " 'in',\n",
              " 'ananthasagar',\n",
              " 'villag',\n",
              " 'of',\n",
              " 'hasanparthi',\n",
              " 'mandal',\n",
              " 'in',\n",
              " 'warang',\n",
              " ',',\n",
              " 'telangana',\n",
              " ',',\n",
              " 'india',\n",
              " '.',\n",
              " 'it',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acr',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separ',\n",
              " 'hostel',\n",
              " 'facil',\n",
              " 'for',\n",
              " 'boy',\n",
              " 'and',\n",
              " 'girl',\n",
              " '.',\n",
              " 'there',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'librari',\n",
              " 'along',\n",
              " 'with',\n",
              " 'india',\n",
              " 'largest',\n",
              " 'technolog',\n",
              " 'busi',\n",
              " 'incub',\n",
              " '(',\n",
              " 'tbi',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'citi',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpMoJl8A5zwc",
        "outputId": "e947e4e3-55c7-4eca-a759-a2b9fdf44c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> the\n",
            "SR ---> sr\n",
            "University ---> univers\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> locat\n",
            "in ---> in\n",
            "Ananthasagar ---> ananthasagar\n",
            "village ---> villag\n",
            "of ---> of\n",
            "Hasanparthy ---> hasanparthi\n",
            "Mandal ---> mandal\n",
            "in ---> in\n",
            "Warangal ---> warang\n",
            ", ---> ,\n",
            "Telangana ---> telangana\n",
            ", ---> ,\n",
            "India ---> india\n",
            ". ---> .\n",
            "It ---> it\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acr\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> separ\n",
            "hostel ---> hostel\n",
            "facilities ---> facil\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> there\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> huge\n",
            "central ---> central\n",
            "library ---> librari\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> india\n",
            "largest ---> largest\n",
            "Technology ---> technolog\n",
            "Business ---> busi\n",
            "Incubator ---> incub\n",
            "( ---> (\n",
            "TBI ---> tbi\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> citi\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRUniversity=\"\"\"The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.\n",
        "It is in 150 acres, with both separate hostel facilities for boys and girls.\n",
        "There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.\"\"\"\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "Lanc = LancasterStemmer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",Lanc.stem(word))"
      ],
      "metadata": {
        "id": "EshiMW0C5910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdf26bc-568e-4c84-e8f9-244010e35733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> the\n",
            "SR ---> sr\n",
            "University ---> univers\n",
            "campus ---> camp\n",
            "is ---> is\n",
            "located ---> loc\n",
            "in ---> in\n",
            "Ananthasagar ---> ananthasag\n",
            "village ---> vil\n",
            "of ---> of\n",
            "Hasanparthy ---> hasanparthy\n",
            "Mandal ---> mand\n",
            "in ---> in\n",
            "Warangal ---> warang\n",
            ", ---> ,\n",
            "Telangana ---> telangan\n",
            ", ---> ,\n",
            "India ---> ind\n",
            ". ---> .\n",
            "It ---> it\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acr\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> sep\n",
            "hostel ---> hostel\n",
            "facilities ---> facil\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> ther\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> hug\n",
            "central ---> cent\n",
            "library ---> libr\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> india\n",
            "largest ---> largest\n",
            "Technology ---> technolog\n",
            "Business ---> busy\n",
            "Incubator ---> incub\n",
            "( ---> (\n",
            "TBI ---> tbi\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> city\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "regexp = RegexpStemmer('ing|e', min=4)\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRkbvf07q1PY",
        "outputId": "497de680-d1c6-4c43-cda7-99a76ae50a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> The\n",
            "SR ---> SR\n",
            "University ---> Univrsity\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> locatd\n",
            "in ---> in\n",
            "Ananthasagar ---> Ananthasagar\n",
            "village ---> villag\n",
            "of ---> of\n",
            "Hasanparthy ---> Hasanparthy\n",
            "Mandal ---> Mandal\n",
            "in ---> in\n",
            "Warangal ---> Warangal\n",
            ", ---> ,\n",
            "Telangana ---> Tlangana\n",
            ", ---> ,\n",
            "India ---> India\n",
            ". ---> .\n",
            "It ---> It\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acrs\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> sparat\n",
            "hostel ---> hostl\n",
            "facilities ---> facilitis\n",
            "for ---> for\n",
            "boys ---> boys\n",
            "and ---> and\n",
            "girls ---> girls\n",
            ". ---> .\n",
            "There ---> Thr\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> hug\n",
            "central ---> cntral\n",
            "library ---> library\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> Indias\n",
            "largest ---> largst\n",
            "Technology ---> Tchnology\n",
            "Business ---> Businss\n",
            "Incubator ---> Incubator\n",
            "( ---> (\n",
            "TBI ---> TBI\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tir\n",
            "2 ---> 2\n",
            "cities ---> citis\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMwt6ZbDrJlm",
        "outputId": "4d68497d-2d3d-4d68-c802-6ba54c4f30d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> The\n",
            "SR ---> SR\n",
            "University ---> University\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> located\n",
            "in ---> in\n",
            "Ananthasagar ---> Ananthasagar\n",
            "village ---> village\n",
            "of ---> of\n",
            "Hasanparthy ---> Hasanparthy\n",
            "Mandal ---> Mandal\n",
            "in ---> in\n",
            "Warangal ---> Warangal\n",
            ", ---> ,\n",
            "Telangana ---> Telangana\n",
            ", ---> ,\n",
            "India ---> India\n",
            ". ---> .\n",
            "It ---> It\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acre\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> separate\n",
            "hostel ---> hostel\n",
            "facilities ---> facility\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> There\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> huge\n",
            "central ---> central\n",
            "library ---> library\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> Indias\n",
            "largest ---> largest\n",
            "Technology ---> Technology\n",
            "Business ---> Business\n",
            "Incubator ---> Incubator\n",
            "( ---> (\n",
            "TBI ---> TBI\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> city\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bbb8nB29rWF-",
        "outputId": "749f1f79-6219-46b3-9a41-7139cf82664f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WyrE3_NXradv",
        "outputId": "1c0f3b54-8498-445a-cbbf-a2ff6cbf786f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing|e', min=4)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer','WordNetLemmatizer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word),lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSG93NvAreov",
        "outputId": "5cf58285-3d7d-45e7-9e4c-489686c173b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          WordNetLemmatizer                                 \n",
            "friend              friend              friend              friend                        frind                                   friend                                            \n",
            "friendship          friendship          friendship          friend                        frindship                               friendship                                        \n",
            "friends             friend              friend              friend                        frinds                                  friend                                            \n",
            "friendships         friendship          friendship          friend                        frindships                              friendship                                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_work=\"\"\"NLP models are transforming the world rapidly!.\"\"\"\n"
      ],
      "metadata": {
        "id": "hwXB3ATir0bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fw5r80iIsQSg",
        "outputId": "301c4f02-132a-41fb-f416-621ff2903813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP models are transforming the world rapidly!.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(class_work)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_iRcjJMsVRS",
        "outputId": "4d809d89-7a32-4552-d03d-db47b37080a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'are', 'transforming', 'the', 'world', 'rapidly', '!', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D5cE6JOsccI",
        "outputId": "42940d64-2302-45a4-eb25-5ea3ecb65fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(class_work)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5YBn9E0smSe",
        "outputId": "5bdfe2d2-1c6e-473a-f080-0d880b996e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'are', 'transforming', 'the', 'world', 'rapidly', '!', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "  if word.casefold() not in stop_words:\n",
        "    filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9bVgb9gs5z_",
        "outputId": "dfd910bb-e552-4779-b142-a25e8d85fba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'transforming', 'world', 'rapidly', '!', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(class_work)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs-b1bfLtAJ_",
        "outputId": "fe337014-2aa9-485f-8374-5ce8b31a1fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nlp', 'model', 'are', 'transform', 'the', 'world', 'rapidli', '!', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(class_work)\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV05rkeVtSgu",
        "outputId": "f77904ec-bf91-4254-e0bd-9478cef4e895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ---> nlp\n",
            "models ---> model\n",
            "are ---> are\n",
            "transforming ---> transform\n",
            "the ---> the\n",
            "world ---> world\n",
            "rapidly ---> rapid\n",
            "! ---> !\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk import LancasterStemmer\n",
        "Lanc = LancasterStemmer()\n",
        "words = word_tokenize(class_work)\n",
        "for word in words:\n",
        "    print(word,\"--->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMKwSoEJtadT",
        "outputId": "bacb56f8-47ed-4e58-f7d2-feb164157750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ---> nlp\n",
            "models ---> model\n",
            "are ---> ar\n",
            "transforming ---> transform\n",
            "the ---> the\n",
            "world ---> world\n",
            "rapidly ---> rapid\n",
            "! ---> !\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing|e|able', min=4)\n",
        "words = word_tokenize(class_work)\n",
        "for word in words:\n",
        "    print(word,\"--->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVZzNwAotjCh",
        "outputId": "50d51f7a-aa76-45c9-f6b9-6c27478fcec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ---> nlp\n",
            "models ---> model\n",
            "are ---> ar\n",
            "transforming ---> transform\n",
            "the ---> the\n",
            "world ---> world\n",
            "rapidly ---> rapid\n",
            "! ---> !\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTMGdaVyuHoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}