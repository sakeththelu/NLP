{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cldJzTD_5qxL"
      },
      "source": [
        "step1:load the data set using pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJDwqXfY3qYw",
        "outputId": "6e66299d-85f3-40b6-d5f5-d7a2354fa770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             News\n",
            "0   Virat scored century in match\n",
            "1            BJP won in elections\n",
            "2  Bumra took 5 wicket in a match\n",
            "3  Congress form state government\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_excel('/content/LDA-Data.xlsx')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br9CvD3w5yuf"
      },
      "source": [
        "step2:Text preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AujUjT63Mku",
        "outputId": "a24e2301-1ff8-41da-c8bb-264908a91c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             News                  processed_news\n",
            "0   Virat scored century in match   virat scored century in match\n",
            "1            BJP won in elections            bjp won in elections\n",
            "2  Bumra took 5 wicket in a match  bumra took 5 wicket in a match\n",
            "3  Congress form state government  congress form state government\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset from the available Excel file\n",
        "df = pd.read_excel('/content/LDA-Data.xlsx')\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'News' column from the loaded Excel data\n",
        "df['processed_news'] = df['News'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df[['News', 'processed_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU-nOMhG5-GB"
      },
      "source": [
        "word tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ru57u6CJ5oPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33828337-964c-45d9-8b7c-e549bcf3d70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   processed_news                          tokenized_news\n",
            "0   virat scored century in match     [virat, scored, century, in, match]\n",
            "1            bjp won in elections               [bjp, won, in, elections]\n",
            "2  bumra took 5 wicket in a match  [bumra, took, 5, wicket, in, a, match]\n",
            "3  congress form state government     [congress, form, state, government]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df['tokenized_news'] = df['processed_news'].apply(lambda x: word_tokenize(x))\n",
        "print(df[['processed_news', 'tokenized_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2trc8xk6TyJ"
      },
      "source": [
        "stop word removal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xUspO6Ar6Lrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f033fee-3815-4ee7-e692-b63d6d4f4dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           tokenized_news                        filtered_news\n",
            "0     [virat, scored, century, in, match]      [virat, scored, century, match]\n",
            "1               [bjp, won, in, elections]                     [bjp, elections]\n",
            "2  [bumra, took, 5, wicket, in, a, match]      [bumra, took, 5, wicket, match]\n",
            "3     [congress, form, state, government]  [congress, form, state, government]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the stopwords resource is available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_news\n",
        "df['filtered_news'] = df['tokenized_news'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_news', 'filtered_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAM18-iS6hQW"
      },
      "source": [
        "lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z56WmIlb6lct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65f0348-0fd3-456f-d9e2-65660e226ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         filtered_news                      lemmatized_news\n",
            "0      [virat, scored, century, match]      [virat, scored, century, match]\n",
            "1                     [bjp, elections]                      [bjp, election]\n",
            "2      [bumra, took, 5, wicket, match]      [bumra, took, 5, wicket, match]\n",
            "3  [congress, form, state, government]  [congress, form, state, government]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df['lemmatized_news'] = df['filtered_news'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['filtered_news', 'lemmatized_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXZfONJt6yLz"
      },
      "source": [
        "Rejoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qM8Ue2z960N_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb465cb-2fea-4ad5-f1a4-07988f9953ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       lemmatized_news                      clean_news\n",
            "0      [virat, scored, century, match]      virat scored century match\n",
            "1                      [bjp, election]                    bjp election\n",
            "2      [bumra, took, 5, wicket, match]       bumra took 5 wicket match\n",
            "3  [congress, form, state, government]  congress form state government\n"
          ]
        }
      ],
      "source": [
        "# Rejoin lemmatized words into a single string\n",
        "df['clean_news'] = df['lemmatized_news'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['lemmatized_news', 'clean_news']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sh2xcXa7D7d"
      },
      "source": [
        "step4: BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7IMikcR-7GjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e1c846-2eae-4ee9-d8d3-e3aec3085ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['bjp' 'bumra' 'century' 'congress' 'election' 'form' 'government' 'match'\n",
            " 'scored' 'state' 'took' 'virat' 'wicket']\n",
            "Bag-of-Words Matrix:\n",
            " [[0 0 1 0 0 0 0 1 1 0 0 1 0]\n",
            " [1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 1]\n",
            " [0 0 0 1 0 1 1 0 0 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform sentences\n",
        "bow_matrix = vectorizer.fit_transform(df['clean_news'])\n",
        "\n",
        "# Convert to array for readability\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Show vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Show Bag-of-Words matrix\n",
        "print(\"Bag-of-Words Matrix:\\n\", bow_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDS6qZOB2c7H"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZUppPgV32EUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c495d9-db96-4e77-f3be-79673685a80c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         Sentence  bjp  bumra  century  congress  election  \\\n",
            "0      virat scored century match    0      0        1         0         0   \n",
            "1                    bjp election    1      0        0         0         1   \n",
            "2       bumra took 5 wicket match    0      1        0         0         0   \n",
            "3  congress form state government    0      0        0         1         0   \n",
            "\n",
            "   form  government  match  scored  state  took  virat  wicket  \n",
            "0     0           0      1       1      0     0      1       0  \n",
            "1     0           0      0       0      0     0      0       0  \n",
            "2     0           0      1       0      0     1      0       1  \n",
            "3     1           1      0       0      1     0      0       0  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform sentences\n",
        "bow_matrix = vectorizer.fit_transform(df['clean_news'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add original sentences for reference\n",
        "bow_df.insert(0, \"Sentence\", df['clean_news'])\n",
        "\n",
        "# Display Bag-of-Words DataFrame\n",
        "print(bow_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTu3tG1Y4EZT"
      },
      "source": [
        "step5: Apply LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oXEZ73VF3Qey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad561d7-9108-4426-adaa-168bcf6bba54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim import corpora\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(df['lemmatized_news'])\n",
        "\n",
        "# Create corpus (bag-of-words representation)\n",
        "corpus = [dictionary.doc2bow(text) for text in df['lemmatized_news']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fwM_0lJp31Vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7caa3156-797e-41dd-dee5-c2b8bc67d0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.199*\"bjp\" + 0.199*\"election\" + 0.050*\"century\" + 0.050*\"virat\" + 0.050*\"scored\"\n",
            "Topic 1: 0.154*\"congress\" + 0.154*\"form\" + 0.154*\"state\" + 0.154*\"government\" + 0.039*\"bjp\"\n",
            "Topic 2: 0.171*\"match\" + 0.098*\"bumra\" + 0.098*\"5\" + 0.098*\"wicket\" + 0.098*\"took\"\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42, passes=10)\n",
        "\n",
        "# Print topics\n",
        "for idx, topic in lda_model.print_topics(num_words=5):\n",
        "    print(f\"Topic {idx}: {topic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cxkxGk04sce"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l81hdeC-38O3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d62ed9-4d65-4dee-f5d9-3298538dbade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic 0:\n",
            "  bjp (0.199)\n",
            "  election (0.199)\n",
            "  century (0.050)\n",
            "  virat (0.050)\n",
            "  scored (0.050)\n",
            "  match (0.050)\n",
            "  took (0.050)\n",
            "  bumra (0.050)\n",
            "  5 (0.050)\n",
            "  wicket (0.050)\n",
            "\n",
            "Topic 1:\n",
            "  congress (0.154)\n",
            "  form (0.154)\n",
            "  state (0.154)\n",
            "  government (0.154)\n",
            "  bjp (0.039)\n",
            "  virat (0.039)\n",
            "  election (0.039)\n",
            "  century (0.039)\n",
            "  match (0.039)\n",
            "  scored (0.039)\n",
            "\n",
            "Topic 2:\n",
            "  match (0.171)\n",
            "  bumra (0.098)\n",
            "  5 (0.098)\n",
            "  wicket (0.098)\n",
            "  took (0.098)\n",
            "  scored (0.097)\n",
            "  virat (0.097)\n",
            "  century (0.097)\n",
            "  election (0.024)\n",
            "  bjp (0.024)\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "# Train LDA model (example with 3 topics)\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42, passes=10)\n",
        "\n",
        "# Print top words for each topic\n",
        "num_words = 10  # how many words per topic\n",
        "topics = lda_model.show_topics(num_topics=3, num_words=num_words, formatted=False)\n",
        "\n",
        "for topic_num, words in topics:\n",
        "    print(f\"\\nTopic {topic_num}:\")\n",
        "    for word, weight in words:\n",
        "        print(f\"  {word} ({weight:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcSi2gIpUbMP"
      },
      "source": [
        "# TASK 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxENqPY4dNP",
        "outputId": "da767cea-242d-4809-8752-b527d83e6ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              titles  \\\n",
            "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
            "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
            "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
            "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
            "4  Background-Foreground Segmentation for Interio...   \n",
            "\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                         terms  \n",
            "0           ['cs.CV', 'cs.LG']  \n",
            "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
            "2           ['cs.CV', 'cs.AI']  \n",
            "3                    ['cs.CV']  \n",
            "4           ['cs.CV', 'cs.LG']  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/arxiv_data.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s04y3TerUywS",
        "outputId": "79e91716-90e4-4843-c0a8-9182b98d5fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                      processed_news  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset from the available CSV file\n",
        "df = pd.read_csv('/content/arxiv_data.csv')\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'summaries' column from the loaded CSV data\n",
        "df['processed_news'] = df['summaries'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df[['summaries', 'processed_news']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtxE5hiTVFnK",
        "outputId": "be311c04-9e79-4555-a5f6-c7e91a16f8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      processed_news  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                      tokenized_news  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df['tokenized_news'] = df['processed_news'].apply(lambda x: word_tokenize(x))\n",
        "print(df[['processed_news', 'tokenized_news']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMW8F-2lVx40",
        "outputId": "a04523ba-698f-429c-8347-c54772cac903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokenized_news  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                       filtered_news  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the stopwords resource is available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_news\n",
        "df['filtered_news'] = df['tokenized_news'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_news', 'filtered_news']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4zJbILEV7BJ",
        "outputId": "8c7e1bb6-08f5-4a73-855c-b2ed958cec6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       filtered_news  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                     lemmatized_news  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df['lemmatized_news'] = df['filtered_news'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['filtered_news', 'lemmatized_news']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR6A4USeW1T6",
        "outputId": "4f108c62-a6d0-4a0b-caad-8d97a0c35b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     lemmatized_news  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                          clean_news  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ],
      "source": [
        "# Rejoin lemmatized words into a single string\n",
        "df['clean_news'] = df['lemmatized_news'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['lemmatized_news', 'clean_news']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cIyRvJpXExo"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform sentences\n",
        "bow_matrix = vectorizer.fit_transform(df['clean_news'])\n",
        "\n",
        "# Convert to array for readability\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Show vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Show Bag-of-Words matrix\n",
        "print(\"Bag-of-Words Matrix:\\n\", bow_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCbiorBz6hM9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}